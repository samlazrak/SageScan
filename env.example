# Local LLM Configuration for ScanSage
# Copy this file to .env and configure your local LLM setup

# LLM Provider Configuration
# Options: "openai", "anthropic", "local", "ollama", "llama-cpp"
DEFAULT_LLM_PROVIDER=local
DEFAULT_LLM_MODEL=llama2

# Local LLM Server URLs
# Ollama server (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# llama.cpp server (default: http://localhost:8080)
LLAMA_CPP_BASE_URL=http://localhost:8080

# API Keys (only needed if using cloud providers)
# OpenAI API Key (for GPT models)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API Key (for Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Application Configuration
LOG_LEVEL=INFO

# File paths
UPLOAD_DIR=./uploads
RESULTS_DIR=./results
DATA_DIR=./data

# Tesseract configuration
TESSERACT_CMD=tesseract

# Model configuration
MAX_TEXT_LENGTH=4000
TEMPERATURE=0.3
MAX_TOKENS=500 